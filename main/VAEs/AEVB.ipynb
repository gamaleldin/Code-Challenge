{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of AEVB as in the original paper for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_z = 2 #dimensionality of latent space\n",
    "n_hid = 100 #number of hidden layers for both the encoder and decoder networks (could be changed to 2 numbers)\n",
    "M = 100 #minibatch size\n",
    "L = 1 #number of posterior samples used per data point, can't be changed yet\n",
    "im_size = 784 #total number of pixels per image, 784 for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, im_size])\n",
    "epsilon = tf.placeholder(tf.float32, [None, n_z])\n",
    "\n",
    "W_theta1 = tf.Variable(tf.zeros([n_z, n_hid]))\n",
    "b_theta1 = tf.Variable(tf.zeros([n_hid]))\n",
    "W_theta2 = tf.Variable(tf.zeros([n_hid, 2*im_size])) #the output of the network parametrizes a 784 dimensional normal\n",
    "#with diagonal covariance matrix, the first half corresponds to the means, the second half to variances\n",
    "b_theta2 = tf.Variable(tf.zeros([2*im_size]))\n",
    "\n",
    "W_phi1 = tf.Variable(tf.zeros([im_size, n_hid]))\n",
    "b_phi1 = tf.Variable(tf.zeros([n_hid]))\n",
    "W_phi2 = tf.Variable(tf.zeros([n_hid, n_z*2])) #the output of the network parametrizes a n_z dimensional normal\n",
    "#with diagonal covariance matrix, the first half corresponds to the means, the second half to variances\n",
    "b_phi2 = tf.Variable(tf.zeros([n_z*2]))\n",
    "\n",
    "hid_phi = tf.nn.tanh(tf.matmul(x, W_phi1) + b_phi1)\n",
    "out_phi = tf.matmul(hid_phi,W_phi2) + b_phi2\n",
    "mu_phi = out_phi[:,:n_z]\n",
    "sigma2_phi = tf.exp(out_phi[:,n_z:])\n",
    "\n",
    "z = mu_phi + tf.sqrt(sigma2_phi) * epsilon\n",
    "hid_theta = tf.nn.tanh(tf.matmul(z, W_theta1) + b_theta1)\n",
    "out_theta = tf.matmul(hid_theta,W_theta2) + b_theta2\n",
    "mu_theta = tf.nn.sigmoid(out_theta[:,:im_size])\n",
    "sigma2_theta = tf.exp(out_theta[:,im_size:])\n",
    "\n",
    "log_p_theta = -(x - mu_theta) * (x - mu_theta) / (2 * sigma2_theta) - 0.5 * tf.log(2 * np.pi * sigma2_theta)\n",
    "L = 0.5 * (tf.reduce_sum(1 + tf.log(sigma2_phi) - mu_phi**2 - sigma2_phi)) / M + tf.reduce_mean(log_p_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdagradOptimizer(0.1).minimize(-L)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training loss -1.0343\n",
      "step 1000, training loss -0.972746\n",
      "step 2000, training loss -0.913026\n",
      "step 3000, training loss -0.857023\n",
      "step 4000, training loss -0.803075\n",
      "step 5000, training loss -0.75061\n",
      "step 6000, training loss -0.699883\n",
      "step 7000, training loss -0.650618\n",
      "step 8000, training loss -0.602197\n",
      "step 9000, training loss -0.554852\n",
      "step 10000, training loss -0.509116\n",
      "step 11000, training loss -0.460539\n",
      "step 12000, training loss -0.418537\n",
      "step 13000, training loss -0.370459\n",
      "step 14000, training loss -0.330528\n",
      "step 15000, training loss -0.283438\n",
      "step 16000, training loss -0.240333\n",
      "step 17000, training loss -0.20009\n",
      "step 18000, training loss -0.15463\n",
      "step 19000, training loss -0.106889\n",
      "step 20000, training loss -0.060435\n",
      "step 21000, training loss -0.0265837\n",
      "step 22000, training loss 0.0106517\n",
      "step 23000, training loss 0.0406468\n",
      "step 24000, training loss 0.093221\n",
      "step 25000, training loss 0.120201\n",
      "step 26000, training loss 0.173568\n",
      "step 27000, training loss 0.189283\n",
      "step 28000, training loss 0.210562\n",
      "step 29000, training loss 0.261959\n",
      "step 30000, training loss 0.290909\n",
      "step 31000, training loss 0.326344\n",
      "step 32000, training loss 0.356053\n",
      "step 33000, training loss 0.398245\n",
      "step 34000, training loss 0.418322\n",
      "step 35000, training loss 0.460448\n",
      "step 36000, training loss 0.456418\n",
      "step 37000, training loss 0.476399\n",
      "step 38000, training loss 0.517392\n",
      "step 39000, training loss 0.567459\n",
      "step 40000, training loss 0.611881\n",
      "step 41000, training loss 0.58246\n",
      "step 42000, training loss 0.628659\n",
      "step 43000, training loss 0.674881\n",
      "step 44000, training loss 0.720763\n",
      "step 45000, training loss 0.706741\n",
      "step 46000, training loss 0.733016\n",
      "step 47000, training loss 0.757188\n",
      "step 48000, training loss 0.749669\n",
      "step 49000, training loss 0.77344\n",
      "step 50000, training loss 0.813985\n",
      "step 51000, training loss 0.841438\n",
      "step 52000, training loss 0.841311\n",
      "step 53000, training loss 0.891063\n",
      "step 54000, training loss 0.864755\n",
      "step 55000, training loss 0.910488\n",
      "step 56000, training loss 0.878062\n",
      "step 57000, training loss 0.95818\n",
      "step 58000, training loss 1.00775\n",
      "step 59000, training loss 0.918788\n",
      "step 60000, training loss 1.00355\n",
      "step 61000, training loss 1.04422\n",
      "step 62000, training loss 0.894704\n",
      "step 63000, training loss 0.889431\n",
      "step 64000, training loss 1.10931\n",
      "step 65000, training loss 1.10334\n",
      "step 66000, training loss 1.1534\n",
      "step 67000, training loss 1.11079\n",
      "step 68000, training loss 0.922268\n",
      "step 69000, training loss 1.16845\n",
      "step 70000, training loss 1.03715\n",
      "step 71000, training loss 1.15404\n",
      "step 72000, training loss 1.13583\n",
      "step 73000, training loss 1.24464\n",
      "step 74000, training loss 1.19018\n",
      "step 75000, training loss 1.15744\n",
      "step 76000, training loss 1.28794\n",
      "step 77000, training loss 1.19246\n",
      "step 78000, training loss 1.25434\n",
      "step 79000, training loss 1.17747\n",
      "step 80000, training loss 1.14857\n",
      "step 81000, training loss 1.30889\n",
      "step 82000, training loss 1.20544\n",
      "step 83000, training loss 1.27028\n",
      "step 84000, training loss 1.29022\n",
      "step 85000, training loss 1.29174\n",
      "step 86000, training loss 1.33834\n",
      "step 87000, training loss 1.37322\n",
      "step 88000, training loss 1.08966\n",
      "step 89000, training loss 1.35363\n",
      "step 90000, training loss 1.40161\n",
      "step 91000, training loss 1.36536\n",
      "step 92000, training loss 1.36818\n",
      "step 93000, training loss 1.37012\n",
      "step 94000, training loss 1.41906\n",
      "step 95000, training loss 0.99885\n",
      "step 96000, training loss 1.46351\n",
      "step 97000, training loss 1.43284\n",
      "step 98000, training loss 1.44323\n",
      "step 99000, training loss 1.48899\n",
      "step 100000, training loss 1.36774\n",
      "step 101000, training loss 1.39883\n",
      "step 102000, training loss 1.41738\n",
      "step 103000, training loss 1.47749\n",
      "step 104000, training loss 1.4392\n",
      "step 105000, training loss 1.55978\n",
      "step 106000, training loss 1.50917\n",
      "step 107000, training loss 1.38583\n",
      "step 108000, training loss 1.56809\n",
      "step 109000, training loss 1.46857\n",
      "step 110000, training loss 1.4919\n",
      "step 111000, training loss 1.61026\n",
      "step 112000, training loss 1.56622\n",
      "step 113000, training loss 1.58365\n",
      "step 114000, training loss 1.49998\n",
      "step 115000, training loss 1.44451\n",
      "step 116000, training loss 1.64656\n",
      "step 117000, training loss 1.58668\n",
      "step 118000, training loss 1.37118\n",
      "step 119000, training loss 1.63255\n",
      "step 120000, training loss 1.67278\n",
      "step 121000, training loss 1.26634\n",
      "step 122000, training loss 1.58246\n",
      "step 123000, training loss 1.67585\n",
      "step 124000, training loss 1.68442\n",
      "step 125000, training loss 1.6363\n",
      "step 126000, training loss 1.51373\n",
      "step 127000, training loss 1.61174\n",
      "step 128000, training loss 1.64583\n",
      "step 129000, training loss 1.71016\n",
      "step 130000, training loss 1.75713\n",
      "step 131000, training loss 1.53725\n",
      "step 132000, training loss 1.66672\n",
      "step 133000, training loss 1.64433\n",
      "step 134000, training loss 1.72511\n",
      "step 135000, training loss 1.66172\n",
      "step 136000, training loss 1.71238\n",
      "step 137000, training loss 1.6591\n",
      "step 138000, training loss 1.77272\n",
      "step 139000, training loss 1.69997\n",
      "step 140000, training loss 1.73656\n",
      "step 141000, training loss 1.73714\n",
      "step 142000, training loss 1.67522\n",
      "step 143000, training loss 1.77571\n",
      "step 144000, training loss 1.73122\n",
      "step 145000, training loss 1.78338\n",
      "step 146000, training loss 1.74497\n",
      "step 147000, training loss 1.79631\n",
      "step 148000, training loss 1.77977\n",
      "step 149000, training loss 1.74685\n",
      "step 150000, training loss 1.72673\n",
      "step 151000, training loss 1.26368\n",
      "step 152000, training loss 1.8414\n",
      "step 153000, training loss 1.54067\n",
      "step 154000, training loss 1.84872\n",
      "step 155000, training loss 1.8982\n",
      "step 156000, training loss 1.86345\n",
      "step 157000, training loss 1.8532\n",
      "step 158000, training loss 1.85362\n",
      "step 159000, training loss 1.84408\n",
      "step 160000, training loss 1.79239\n",
      "step 161000, training loss 1.82223\n",
      "step 162000, training loss 1.49419\n",
      "step 163000, training loss 1.85164\n",
      "step 164000, training loss 1.73906\n",
      "step 165000, training loss 1.90457\n",
      "step 166000, training loss 1.92891\n",
      "step 167000, training loss 1.65593\n",
      "step 168000, training loss 1.87794\n",
      "step 169000, training loss 1.73572\n",
      "step 170000, training loss 1.85555\n",
      "step 171000, training loss 1.85735\n",
      "step 172000, training loss 1.72625\n",
      "step 173000, training loss 1.87319\n",
      "step 174000, training loss 1.84392\n",
      "step 175000, training loss 1.98372\n",
      "step 176000, training loss 1.79139\n",
      "step 177000, training loss 1.69192\n",
      "step 178000, training loss 1.81575\n",
      "step 179000, training loss 1.94909\n",
      "step 180000, training loss 2.01233\n",
      "step 181000, training loss 1.94912\n",
      "step 182000, training loss 2.03138\n",
      "step 183000, training loss 1.80245\n",
      "step 184000, training loss 2.02324\n",
      "step 185000, training loss 1.9427\n",
      "step 186000, training loss 1.95547\n",
      "step 187000, training loss 2.021\n",
      "step 188000, training loss 2.03947\n",
      "step 189000, training loss 1.92332\n",
      "step 190000, training loss 1.79813\n",
      "step 191000, training loss 2.05062\n",
      "step 192000, training loss 2.0827\n",
      "step 193000, training loss 1.9734\n",
      "step 194000, training loss 2.06671\n",
      "step 195000, training loss 2.07616\n",
      "step 196000, training loss 2.06687\n",
      "step 197000, training loss 1.90277\n",
      "step 198000, training loss 2.14229\n",
      "step 199000, training loss 2.09154\n",
      "step 200000, training loss 2.02945\n",
      "step 201000, training loss 2.13145\n",
      "step 202000, training loss 2.111\n",
      "step 203000, training loss 2.15805\n",
      "step 204000, training loss 1.93077\n",
      "step 205000, training loss 2.11535\n",
      "step 206000, training loss 2.04613\n",
      "step 207000, training loss 2.17237\n",
      "step 208000, training loss 2.16349\n",
      "step 209000, training loss 2.2192\n",
      "step 210000, training loss 2.15536\n",
      "step 211000, training loss 2.1326\n",
      "step 212000, training loss 2.18299\n",
      "step 213000, training loss 2.19269\n",
      "step 214000, training loss 2.06254\n",
      "step 215000, training loss 2.21289\n",
      "step 216000, training loss 2.05859\n",
      "step 217000, training loss 2.12088\n",
      "step 218000, training loss 2.24053\n",
      "step 219000, training loss 2.06492\n",
      "step 220000, training loss 2.16648\n",
      "step 221000, training loss 2.23106\n",
      "step 222000, training loss 2.12678\n",
      "step 223000, training loss 2.27692\n",
      "step 224000, training loss 2.29981\n",
      "step 225000, training loss 2.27311\n",
      "step 226000, training loss 2.23271\n",
      "step 227000, training loss 2.12866\n",
      "step 228000, training loss 2.25647\n",
      "step 229000, training loss 2.3235\n",
      "step 230000, training loss 2.13456\n",
      "step 231000, training loss 1.76294\n",
      "step 232000, training loss 2.31813\n",
      "step 233000, training loss 2.25652\n",
      "step 234000, training loss 2.27455\n",
      "step 235000, training loss 2.36616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 236000, training loss 2.37582\n",
      "step 237000, training loss 2.35216\n",
      "step 238000, training loss 2.35885\n",
      "step 239000, training loss 2.25925\n",
      "step 240000, training loss 2.11354\n",
      "step 241000, training loss 2.29001\n",
      "step 242000, training loss 2.36732\n",
      "step 243000, training loss 2.38829\n",
      "step 244000, training loss 2.01679\n",
      "step 245000, training loss 2.35955\n",
      "step 246000, training loss 2.39422\n",
      "step 247000, training loss 2.3288\n",
      "step 248000, training loss 2.37136\n",
      "step 249000, training loss 2.38752\n",
      "step 250000, training loss 2.3964\n",
      "step 251000, training loss 2.42596\n",
      "step 252000, training loss 2.37359\n",
      "step 253000, training loss 2.35281\n",
      "step 254000, training loss 2.41663\n",
      "step 255000, training loss 2.19084\n",
      "step 256000, training loss 2.40789\n",
      "step 257000, training loss 2.40105\n",
      "step 258000, training loss 1.83623\n",
      "step 259000, training loss 2.3633\n",
      "step 260000, training loss 2.39442\n",
      "step 261000, training loss 2.44173\n",
      "step 262000, training loss 2.37102\n",
      "step 263000, training loss 2.39164\n",
      "step 264000, training loss 2.46952\n",
      "step 265000, training loss 2.44643\n",
      "step 266000, training loss 2.50807\n",
      "step 267000, training loss 2.52249\n",
      "step 268000, training loss 2.48381\n",
      "step 269000, training loss 2.51878\n",
      "step 270000, training loss 2.47267\n",
      "step 271000, training loss 2.42517\n",
      "step 272000, training loss 2.49926\n",
      "step 273000, training loss 2.53823\n",
      "step 274000, training loss 2.49167\n",
      "step 275000, training loss 2.54466\n",
      "step 276000, training loss 2.54931\n",
      "step 277000, training loss 2.60574\n",
      "step 278000, training loss 2.44671\n",
      "step 279000, training loss 2.59304\n",
      "step 280000, training loss 2.33177\n",
      "step 281000, training loss 2.56645\n",
      "step 282000, training loss 2.63115\n",
      "step 283000, training loss 2.54093\n",
      "step 284000, training loss 2.64434\n",
      "step 285000, training loss 2.50633\n",
      "step 286000, training loss 2.15882\n",
      "step 287000, training loss 2.25623\n",
      "step 288000, training loss 2.61833\n",
      "step 289000, training loss 2.44945\n",
      "step 290000, training loss 2.62272\n",
      "step 291000, training loss 2.58067\n",
      "step 292000, training loss 2.51276\n",
      "step 293000, training loss 2.64653\n",
      "step 294000, training loss 2.67469\n",
      "step 295000, training loss 2.5986\n",
      "step 296000, training loss 2.6192\n",
      "step 297000, training loss 2.6485\n",
      "step 298000, training loss 2.60766\n",
      "step 299000, training loss 2.68321\n",
      "step 300000, training loss 1.98572\n",
      "step 301000, training loss 2.74983\n",
      "step 302000, training loss 2.71413\n",
      "step 303000, training loss 2.6863\n",
      "step 304000, training loss 2.71142\n",
      "step 305000, training loss 2.69148\n",
      "step 306000, training loss 2.61445\n",
      "step 307000, training loss 2.7155\n",
      "step 308000, training loss 2.77084\n",
      "step 309000, training loss 2.71686\n",
      "step 310000, training loss 2.34361\n",
      "step 311000, training loss 2.7389\n",
      "step 312000, training loss 2.58231\n",
      "step 313000, training loss 2.76915\n",
      "step 314000, training loss 2.73059\n",
      "step 315000, training loss 2.74604\n",
      "step 316000, training loss 2.79587\n",
      "step 317000, training loss 2.76302\n",
      "step 318000, training loss 2.80127\n",
      "step 319000, training loss 2.78882\n",
      "step 320000, training loss 2.72216\n",
      "step 321000, training loss 2.82799\n",
      "step 322000, training loss 2.76363\n",
      "step 323000, training loss 2.73143\n",
      "step 324000, training loss 2.84868\n",
      "step 325000, training loss 2.85221\n",
      "step 326000, training loss 2.75102\n",
      "step 327000, training loss 2.89164\n",
      "step 328000, training loss 2.7955\n",
      "step 329000, training loss 2.8007\n",
      "step 330000, training loss 2.87523\n",
      "step 331000, training loss 2.84942\n",
      "step 332000, training loss 2.87718\n",
      "step 333000, training loss 2.55077\n",
      "step 334000, training loss 2.83218\n",
      "step 335000, training loss 2.85885\n",
      "step 336000, training loss 2.61485\n",
      "step 337000, training loss 2.9129\n",
      "step 338000, training loss 2.85724\n",
      "step 339000, training loss 2.87469\n",
      "step 340000, training loss 2.79235\n",
      "step 341000, training loss 2.7839\n",
      "step 342000, training loss 2.71846\n",
      "step 343000, training loss 2.91115\n",
      "step 344000, training loss 2.85737\n",
      "step 345000, training loss 2.9341\n",
      "step 346000, training loss 2.85941\n",
      "step 347000, training loss 2.88591\n",
      "step 348000, training loss 2.73483\n",
      "step 349000, training loss 3.02081\n",
      "step 350000, training loss 2.8078\n",
      "step 351000, training loss 2.51772\n",
      "step 352000, training loss 3.01489\n",
      "step 353000, training loss 3.01299\n",
      "step 354000, training loss 2.95614\n",
      "step 355000, training loss 2.95772\n",
      "step 356000, training loss 3.02373\n",
      "step 357000, training loss 3.05613\n",
      "step 358000, training loss 3.04638\n",
      "step 359000, training loss 2.81894\n",
      "step 360000, training loss 2.90273\n",
      "step 361000, training loss 2.87487\n",
      "step 362000, training loss 3.01517\n",
      "step 363000, training loss 2.92009\n",
      "step 364000, training loss 2.99153\n",
      "step 365000, training loss 2.85795\n",
      "step 366000, training loss 3.01778\n",
      "step 367000, training loss 3.0661\n",
      "step 368000, training loss 3.035\n",
      "step 369000, training loss 3.09091\n",
      "step 370000, training loss 3.06893\n",
      "step 371000, training loss 3.08173\n",
      "step 372000, training loss 3.05572\n",
      "step 373000, training loss 3.02707\n",
      "step 374000, training loss 3.03352\n",
      "step 375000, training loss 2.49266\n",
      "step 376000, training loss 2.93165\n",
      "step 377000, training loss 3.03975\n",
      "step 378000, training loss 3.00885\n",
      "step 379000, training loss 3.02206\n",
      "step 380000, training loss 3.02519\n",
      "step 381000, training loss 3.03244\n",
      "step 382000, training loss 2.91715\n",
      "step 383000, training loss 3.10777\n",
      "step 384000, training loss 3.14447\n",
      "step 385000, training loss 3.13637\n",
      "step 386000, training loss 3.15424\n",
      "step 387000, training loss 3.13064\n",
      "step 388000, training loss 3.14732\n",
      "step 389000, training loss 3.24647\n",
      "step 390000, training loss 3.22141\n",
      "step 391000, training loss 3.19843\n",
      "step 392000, training loss 3.04\n",
      "step 393000, training loss 3.23671\n",
      "step 394000, training loss 3.13591\n",
      "step 395000, training loss 3.1401\n",
      "step 396000, training loss 3.21415\n",
      "step 397000, training loss 3.23072\n",
      "step 398000, training loss 3.2577\n",
      "step 399000, training loss 3.2429\n",
      "step 400000, training loss 3.16789\n",
      "step 401000, training loss 3.2446\n",
      "step 402000, training loss 3.22866\n",
      "step 403000, training loss 3.20514\n",
      "step 404000, training loss 3.10119\n",
      "step 405000, training loss 3.25282\n",
      "step 406000, training loss 2.96082\n",
      "step 407000, training loss 2.89423\n",
      "step 408000, training loss 2.85268\n",
      "step 409000, training loss 3.23138\n",
      "step 410000, training loss 3.16293\n",
      "step 411000, training loss 3.2653\n",
      "step 412000, training loss 3.2738\n",
      "step 413000, training loss 3.30566\n",
      "step 414000, training loss 2.97068\n",
      "step 415000, training loss 3.04162\n",
      "step 416000, training loss 3.3053\n",
      "step 417000, training loss 3.17048\n",
      "step 418000, training loss 3.32052\n",
      "step 419000, training loss 3.3263\n",
      "step 420000, training loss 3.29713\n",
      "step 421000, training loss 3.34627\n",
      "step 422000, training loss 3.30627\n",
      "step 423000, training loss 3.2012\n",
      "step 424000, training loss 3.328\n",
      "step 425000, training loss 3.31952\n",
      "step 426000, training loss 3.32016\n",
      "step 427000, training loss 3.39092\n",
      "step 428000, training loss 3.37237\n",
      "step 429000, training loss 3.32418\n",
      "step 430000, training loss 3.34466\n",
      "step 431000, training loss 3.34014\n",
      "step 432000, training loss 2.99599\n",
      "step 433000, training loss 2.83146\n",
      "step 434000, training loss 3.3532\n",
      "step 435000, training loss 3.4196\n",
      "step 436000, training loss 3.29721\n",
      "step 437000, training loss 3.08695\n",
      "step 438000, training loss 3.40237\n",
      "step 439000, training loss 3.2476\n",
      "step 440000, training loss 3.21494\n",
      "step 441000, training loss 3.07151\n",
      "step 442000, training loss 3.34636\n",
      "step 443000, training loss 3.4692\n",
      "step 444000, training loss 3.37287\n",
      "step 445000, training loss 3.44256\n",
      "step 446000, training loss 3.22216\n",
      "step 447000, training loss 3.45367\n",
      "step 448000, training loss 3.35115\n",
      "step 449000, training loss 3.3404\n",
      "step 450000, training loss 3.41726\n",
      "step 451000, training loss 3.4891\n",
      "step 452000, training loss 3.46094\n",
      "step 453000, training loss 3.38042\n",
      "step 454000, training loss 3.52023\n",
      "step 455000, training loss 3.16391\n",
      "step 456000, training loss 3.34848\n",
      "step 457000, training loss 3.49759\n",
      "step 458000, training loss 3.3773\n",
      "step 459000, training loss 3.49596\n",
      "step 460000, training loss 3.5369\n",
      "step 461000, training loss 3.4888\n",
      "step 462000, training loss 3.5034\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    batch = mnist.train.next_batch(M)\n",
    "    eps = np.random.normal(size=(M,n_z))\n",
    "    if i%1000 == 0:\n",
    "        train_L = sess.run(L, feed_dict={x:batch[0], epsilon:eps})\n",
    "        print(\"step %d, training loss %g\"%(i, train_L))\n",
    "    sess.run(train_step, feed_dict={x: batch[0], epsilon: eps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1202ec590>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122dfa290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_image_vec = mnist.train.next_batch(1)[0]\n",
    "original_image = original_image_vec.reshape(28,28)\n",
    "plt.imshow(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1231012d0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122e14750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eps = np.random.normal(size=(1,n_z))\n",
    "mu = sess.run(mu_theta, feed_dict={x:original_image_vec, epsilon:eps})\n",
    "sigma2 = sess.run(sigma2_theta, feed_dict={x:original_image_vec, epsilon:eps})\n",
    "sim_image = np.random.normal(mu, sigma2).reshape(28,28)\n",
    "plt.imshow(sim_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
